{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pandas as ps\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 96223: expected 1 fields, saw 2\\n'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tom is not as fat as I am.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is it OK if I open a can?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tom said he would be thirteen next month.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I got Tom to do it for me.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Tom is not as fat as I am.\n",
       "0                  Is it OK if I open a can?\n",
       "1  Tom said he would be thirteen next month.\n",
       "2                 I got Tom to do it for me."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = ps.read_csv(os.path.abspath('sentences.csv'), error_bad_lines=False)\n",
    "df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Util():\n",
    "    \n",
    "    def assign_word_count(self, word, dictionary):\n",
    "        if word in dictionary:\n",
    "            dictionary[word] = dictionary[word] + 1\n",
    "        else:\n",
    "            dictionary[word] = 1\n",
    "        return dictionary\n",
    "    \n",
    "util = Util()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix, isTarget):\n",
    "    \n",
    "    idxs = [to_ix[w] if w in to_ix.keys() else to_ix['unk'] for w in seq]\n",
    "    if isTarget:\n",
    "        idxs.append(0)\n",
    "    else:\n",
    "        idxs.append(1) # EOS\n",
    "    return torch.tensor(idxs, dtype=torch.long).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "validation_data = []\n",
    "word_count = {}\n",
    "ix2word = {}\n",
    "word2ix = {}\n",
    "for i, sentence in enumerate(df['Tom is not as fat as I am.']):\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation)) # remove punctuation\n",
    "    sentence = sentence.split() # split in array\n",
    "    sentence = [word.lower() for word in sentence] # lower everything \n",
    "    sentence.append('EOS')\n",
    "    [util.assign_word_count(word, word_count) for word in sentence]\n",
    "    reversed_s = sentence[::-1]\n",
    "    if i < len(df['Tom is not as fat as I am.']) - 1001:\n",
    "        training_data.append((sentence, reversed_s))\n",
    "    else:\n",
    "        validation_data.append((sentence, reversed_s))\n",
    "\n",
    "word2ix['SOS'] = 0\n",
    "word2ix['EOS'] = 1\n",
    "word2ix['unk'] = 2\n",
    "ix2word[0] = 'SOS'\n",
    "ix2word[1] = 'EOS'\n",
    "ix2word[2] = 'unk'\n",
    "\n",
    "for sentence, target in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word2ix and word_count[word] > 5:\n",
    "            word2ix[word] = len(word2ix)\n",
    "            ix2word[len(word2ix) - 1] = word\n",
    "for sentence, target in validation_data:\n",
    "    for word in sentence:\n",
    "        if word not in word2ix and word_count[word] > 5:\n",
    "            word2ix[word] = len(word2ix)\n",
    "            ix2word[len(word2ix) - 1] = word\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__() # initialize the params/methods for nn Module class\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, inputs, hidden):\n",
    "        x = self.embedding(inputs)\n",
    "        #x = self.dropout(x)\n",
    "        x = x.view(1, 1, -1)\n",
    "        out, hidden = self.gru(x)\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(len(word2ix), 255).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size) \n",
    "        self.attn = nn.Linear(2 * hidden_size, 1)\n",
    "        self.last = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n",
    "    \n",
    "    def forward(self, encoder_outputs, decoder_input, decoder_hidden):\n",
    "        alpha = []\n",
    "        for i in range(len(encoder_outputs)):\n",
    "            concat = torch.cat((encoder_outputs[i], decoder_hidden[0]), dim=1) #concat encoder_outputs + decoder_hidden\n",
    "            alpha_element = self.attn(concat) # linear regression the encoder output and hidden to find the attention weight coresponding to encoder output at time t\n",
    "            alpha.append(alpha_element) # append the weight\n",
    "        alpha = torch.cat(alpha,1)\n",
    "        alpha_normalized = F.softmax(alpha,1)\n",
    "        \n",
    "        c = torch.bmm(alpha_normalized.view(1, 1, 10), encoder_outputs.view(1, -1, self.hidden_size))\n",
    "        \n",
    "        embedding = self.embedding(decoder_input[0]).view(1, 1, -1) #embedding of the last decoder output as input\n",
    "    \n",
    "        input_decoder = torch.cat((embedding, c), 2) # concat the (attention_norm | encoder_outputs) to the last input of decoder\n",
    "        \n",
    "        \n",
    "        out, hidden = self.gru(input_decoder, decoder_hidden)\n",
    "\n",
    "        out = self.last(out[0])\n",
    "        out = F.log_softmax(out[0])\n",
    "        \n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "decoder = AttentionDecoder(len(word2ix), 255).cuda()\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer_encoder = optim.Adam(encoder.parameters())\n",
    "optimizer_decoder = optim.Adam(decoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, total, correct):\n",
    "    encoder_hidden = encoder.init_hidden().cuda()\n",
    "    \n",
    "    optimizer_encoder.zero_grad()\n",
    "    optimizer_decoder.zero_grad()\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    encoder_outputs = torch.zeros(10, 1, encoder.hidden_size).cuda()\n",
    "    loss = 0\n",
    "    use_teacher_forcing = True if random.random() < 0.5 else False\n",
    "\n",
    "    \n",
    "    for word in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[word], encoder_hidden)\n",
    "        encoder_outputs[word] = encoder_output[0, 0]\n",
    "    decoder_input = torch.tensor([[0]]).cuda()\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    if not use_teacher_forcing:\n",
    "        for i in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(encoder_outputs, decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.detach().long().cuda()\n",
    "            loss += criterion(decoder_output.view(1,-1), target_tensor[i].unsqueeze(0))\n",
    "            total += 1\n",
    "            if ix2word[decoder_input.item()] == ix2word[target_tensor[i].item()]:\n",
    "                correct += 1\n",
    "            \n",
    "            if decoder_input == 0: # SOS daca e inversat opreste generarea\n",
    "                #print(\"EOS\")\n",
    "                break\n",
    "    else:\n",
    "        for i in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(encoder_outputs, decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output.view(1,-1), target_tensor[i].unsqueeze(0))\n",
    "            decoder_input = target_tensor[i].unsqueeze(0).unsqueeze(1).unsqueeze(1).long().cuda()\n",
    "    \n",
    "    \n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer_encoder.step()\n",
    "    optimizer_decoder.step()\n",
    "    \n",
    "    \n",
    "    return loss, total, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(input_tensor, target_tensor, total_v, correct_v):\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden = encoder.init_hidden().cuda()\n",
    "        encoder_outputs = torch.zeros(10, 1, encoder.hidden_size).cuda()\n",
    "        \n",
    "        for word in range(len(input_tensor)):\n",
    "            encoder_out, encoder_hidden = encoder(input_tensor[word], encoder_hidden)\n",
    "            encoder_outputs[word] = encoder_out[0,0]\n",
    "            \n",
    "        decoder_input = torch.tensor([[0]]).cuda()\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        for word in range(len(input_tensor)):\n",
    "            decoder_output, decoder_hidden = decoder(encoder_outputs, decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.detach().long().cuda()\n",
    "            total_v += 1\n",
    "            if ix2word[decoder_input.item()] == ix2word[target_tensor[word].item()]:\n",
    "                correct_v += 1\n",
    "            if decoder_input == 0:\n",
    "                break\n",
    "                \n",
    "    return total_v, correct_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cordu\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Training results --\n",
      "2.5239410003783584\n",
      "tensor(16.7485, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "-- Validation results --\n",
      "18.061522585585948\n",
      "Epoch 1\n",
      "-- Training results --\n",
      "38.65876891899636\n",
      "tensor(12.0039, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "-- Validation results --\n",
      "28.78440434610935\n",
      "Epoch 2\n"
     ]
    }
   ],
   "source": [
    "def learn():\n",
    "    for i in range(5):\n",
    "        loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        correct_v = 0\n",
    "        total_v = 0\n",
    "        print('Epoch {}'.format(i))\n",
    "        for data, target in training_data:\n",
    "            if len(data) < 10:\n",
    "                loss_it, total_it, correct_it = train(prepare_sequence(target, word2ix, True), prepare_sequence(target, word2ix, True), total, correct)\n",
    "                loss += loss_it\n",
    "                total += total_it\n",
    "                correct += correct_it\n",
    "        for data, target in validation_data:\n",
    "            if len(data) < 10:\n",
    "                total_v_it, correct_v_it = validate(prepare_sequence(data, word2ix, False), prepare_sequence(target, word2ix, True), total_v, correct_v)\n",
    "                total_v += total_v_it\n",
    "                correct_v += correct_v_it\n",
    "        print(\"-- Training results --\")\n",
    "        print(correct * 100 / total)   \n",
    "        print(loss / len(training_data))\n",
    "        print(\"-- Validation results --\")\n",
    "        print(correct_v * 100 / total_v)\n",
    "\n",
    "learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
