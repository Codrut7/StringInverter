{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pandas as ps\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 96223: expected 1 fields, saw 2\\n'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tom is not as fat as I am.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is it OK if I open a can?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tom said he would be thirteen next month.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I got Tom to do it for me.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Tom is not as fat as I am.\n",
       "0                  Is it OK if I open a can?\n",
       "1  Tom said he would be thirteen next month.\n",
       "2                 I got Tom to do it for me."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = ps.read_csv(os.path.abspath('sentences.csv'), error_bad_lines=False)\n",
    "df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Util():\n",
    "    \n",
    "    def assign_word_count(self, word, dictionary):\n",
    "        if word in dictionary:\n",
    "            dictionary[word] = dictionary[word] + 1\n",
    "        else:\n",
    "            dictionary[word] = 1\n",
    "        return dictionary\n",
    "    \n",
    "util = Util()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "word_count = {}\n",
    "ix2word = {}\n",
    "word2ix = {}\n",
    "for i, sentence in enumerate(df['Tom is not as fat as I am.']):\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation)) # remove punctuation\n",
    "    sentence = sentence.split() # split in array\n",
    "    sentence = [word.lower() for word in sentence] # lower everything \n",
    "    sentence.append('EOS')\n",
    "    [util.assign_word_count(word, word_count) for word in sentence]\n",
    "    reversed_s = sentence[::-1]\n",
    "    training_data.append((sentence, reversed_s))\n",
    "    if i == 1500:\n",
    "        break\n",
    "\n",
    "word2ix['SOS'] = 0\n",
    "word2ix['EOS'] = 1\n",
    "word2ix['unk'] = 2\n",
    "ix2word[0] = 'SOS'\n",
    "ix2word[1] = 'EOS'\n",
    "ix2word[2] = 'unk'\n",
    "\n",
    "for sentence, target in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word2ix and word_count[word] > 2:\n",
    "            word2ix[word] = len(word2ix)\n",
    "            ix2word[len(word2ix) - 1] = word\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__() # initialize the params/methods for nn Module class\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, inputs, hidden):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(1, 1, -1)\n",
    "        out, hidden = self.gru(x)\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(len(word2ix), 64).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix, isTarget):\n",
    "    \n",
    "    idxs = [to_ix[w] if w in to_ix.keys() else to_ix['unk'] for w in seq]\n",
    "    if isTarget:\n",
    "        idxs.append(0)\n",
    "    else:\n",
    "        idxs.append(1) # EOS\n",
    "    return torch.tensor(idxs, dtype=torch.long).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size) \n",
    "        self.attn = nn.Linear(2 * hidden_size, 1)\n",
    "        self.last = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n",
    "    \n",
    "    def forward(self, encoder_outputs, decoder_input, decoder_hidden):\n",
    "        alpha = []\n",
    "        for i in range(len(encoder_outputs)):\n",
    "            concat = torch.cat((encoder_outputs[i], decoder_hidden[0]), dim=1) #concat encoder_outputs + decoder_hidden\n",
    "            alpha_element = self.attn(concat) # linear regression the encoder output and hidden to find the attention weight coresponding to encoder output at time t\n",
    "            alpha.append(alpha_element) # append the weight\n",
    "        alpha = torch.cat(alpha,1)\n",
    "        alpha_normalized = F.softmax(alpha,1)\n",
    "        \n",
    "        c = torch.bmm(alpha_normalized.view(1, 1, 10), encoder_outputs.view(1, -1, self.hidden_size))\n",
    "        \n",
    "        embedding = self.embedding(decoder_input[0]).view(1, 1, -1) #embedding of the last decoder output as input\n",
    "    \n",
    "        input_decoder = torch.cat((embedding, c), 2) # concat the (attention_norm | encoder_outputs) to the last input of decoder\n",
    "        \n",
    "        \n",
    "        out, hidden = self.gru(input_decoder, decoder_hidden)\n",
    "        out = self.last(out[0])\n",
    "        \n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = AttentionDecoder(len(word2ix), 64).cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_encoder = optim.SGD(encoder.parameters(), lr=0.1)\n",
    "optimizer_decoder = optim.SGD(decoder.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor):\n",
    "    encoder_hidden = encoder.init_hidden().cuda()\n",
    "    \n",
    "    optimizer_encoder.zero_grad()\n",
    "    optimizer_decoder.zero_grad()\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    encoder_outputs = torch.zeros(10, 1, encoder.hidden_size).cuda()\n",
    "    \n",
    "    loss = 0\n",
    "    use_teacher_forcing = True if random.random() < 0.5 else False\n",
    "    \n",
    "    for word in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[word], encoder_hidden)\n",
    "        encoder_outputs[word] = encoder_output[0, 0]\n",
    "    decoder_input = torch.tensor([[0]]).cuda()\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    if not use_teacher_forcing:\n",
    "        for i in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(encoder_outputs, decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.detach().long().cuda()\n",
    "            loss += criterion(decoder_output.view(1,-1), target_tensor[i].unsqueeze(0))\n",
    "            if decoder_input == 0: # SOS daca e inversat opreste generarea\n",
    "                #print(\"EOS\")\n",
    "                break\n",
    "    else:\n",
    "        for i in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(encoder_outputs, decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output.view(1,-1), target_tensor[i].unsqueeze(0))\n",
    "            decoder_input = target_tensor[i].unsqueeze(0).unsqueeze(1).unsqueeze(1).long().cuda()\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer_encoder.step()\n",
    "    optimizer_decoder.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "tensor(35.1297, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 1\n",
      "tensor(33.4538, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 2\n",
      "tensor(32.4712, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 3\n",
      "tensor(32.3531, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 4\n",
      "tensor(31.9618, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 5\n",
      "tensor(31.4248, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 6\n",
      "tensor(31.8215, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 7\n",
      "tensor(31.7564, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 8\n",
      "tensor(31.1349, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 9\n",
      "tensor(30.9218, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 10\n",
      "tensor(31.0891, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 11\n",
      "tensor(30.6863, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 12\n",
      "tensor(30.8412, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 13\n",
      "tensor(32.0315, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 14\n",
      "tensor(30.8318, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 15\n",
      "tensor(31.1510, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 16\n",
      "tensor(30.8532, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 17\n",
      "tensor(31.2819, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 18\n",
      "tensor(32.4766, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 19\n",
      "tensor(34.1187, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 20\n",
      "tensor(35.9739, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 21\n",
      "tensor(35.1130, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 22\n",
      "tensor(35.9006, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 23\n",
      "tensor(37.4006, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 24\n",
      "tensor(36.2283, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 25\n",
      "tensor(36.9506, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 26\n",
      "tensor(37.6255, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 27\n",
      "tensor(36.9515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 28\n",
      "tensor(36.2594, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 29\n",
      "tensor(36.1345, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 30\n",
      "tensor(36.1896, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 31\n",
      "tensor(36.2780, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 32\n",
      "tensor(36.6441, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 33\n",
      "tensor(36.6832, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 34\n",
      "tensor(36.6282, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 35\n",
      "tensor(36.8108, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 36\n",
      "tensor(36.7731, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 37\n",
      "tensor(36.6330, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 38\n",
      "tensor(56.2917, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 39\n",
      "tensor(53.4842, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 40\n",
      "tensor(52.5379, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 41\n",
      "tensor(51.3404, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 42\n",
      "tensor(49.6442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 43\n",
      "tensor(51.1133, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 44\n",
      "tensor(50.0972, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 45\n",
      "tensor(49.2948, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 46\n",
      "tensor(48.9592, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 47\n",
      "tensor(50.3153, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 48\n",
      "tensor(49.9675, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 49\n",
      "tensor(50.2124, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def learn():\n",
    "    for i in range(50):\n",
    "        loss = 0\n",
    "        print('Epoch {}'.format(i))\n",
    "        for data, target in training_data:\n",
    "            loss += train(prepare_sequence(data, word2ix, False), prepare_sequence(target, word2ix, True))\n",
    "        print(loss / len(training_data))\n",
    "            \n",
    "learn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOS\n",
      "that\n",
      "do\n",
      "to\n"
     ]
    }
   ],
   "source": [
    "sequence = prepare_sequence('i like you'.split(), word2ix, False).cuda()\n",
    "\n",
    "encoder_hidden = encoder.init_hidden()\n",
    "decoder_hidden = decoder.init_hidden()\n",
    "\n",
    "encoder_outputs = torch.zeros(10, 1, encoder.hidden_size).float().cuda()\n",
    "with torch.no_grad():\n",
    "    for word in range(len(sequence)):\n",
    "        encoder_output, encoder_hidden = encoder(sequence[word], encoder_hidden)\n",
    "        encoder_outputs[word] = encoder_output[0, 0]\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_input = torch.tensor([[0]]).cuda()\n",
    "    for word in range(len(sequence)):\n",
    "        decoder_output, decoder_hidden = decoder(encoder_outputs, decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.unsqueeze(1).unsqueeze(1).detach().long()\n",
    "        print(ix2word[topi.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
